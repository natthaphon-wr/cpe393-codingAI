{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natthaphon-wr/cpe393-codingAI/blob/main/Lab8_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jomGrnTqvzY2"
      },
      "source": [
        "# Lab 8: Training Deep Recurrent Neural Network - Part 2\n",
        "Natthaphon Rotechanathamcharoen 62070501019 <br>\n",
        "\n",
        "**Note: Please name your file**\n",
        "\n",
        "## Lab Instruction - Language Modelling and Text Classification\n",
        "\n",
        "In this lab, you will learn to train a deep recurrent neural network using LSTM with the Keras library using the Tensorflow backend. Your task is to implement the natural language modelling and text generation.\n",
        "\n",
        "Select your favourite book from https://www.gutenberg.org/browse/scores/top and download it as a text file. Then, you will train your language model using RNN-LSTM. \n",
        "\n",
        "- Language model (in Thai): http://bit.ly/language_model_1\n",
        "- Tutorial on how to create a language model (in English): https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n",
        "\n",
        "To evaluate the model, the perplexity measurement is used: https://stats.stackexchange.com/questions/10302/what-is-perplexity\n",
        "\n",
        "Last, fine-tune your model. You have to try different hyperparameter or adding more data. Discuss your result.\n",
        "\n",
        "\n",
        "\n",
        "**The total lab score is 15 which will be evaluated as follows:**</br>\n",
        "1. Specification (Do as the instruction said. This include the model tuning section where you have to do a proper amount of tuning) - 7 points\n",
        "2. Design of logic (No weired things in the process) -  4points\n",
        "3. Journaling (Communicate your thought process, comment your code, and discuss result & analyse **in every step**) - 4 points\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import Library and Custom Function"
      ],
      "metadata": {
        "id": "BZr_DACx161f"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcWCFsnfvzY7"
      },
      "source": [
        "# Import require library\n",
        "from keras import *\n",
        "from keras.preprocessing import text\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import _utils as fn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras import models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from keras.layers import Embedding \n",
        "#from keras.layers import LSTM\n",
        "#from keras.layers import Dropout \n",
        "#from keras.layers import Dense\n",
        "from keras import layers\n",
        "import keras.backend "
      ],
      "metadata": {
        "id": "lhsLmky-a3bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDOBkmNYvzZf"
      },
      "source": [
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = keras.backend.categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = keras.backend.pow(2.0, cross_entropy)\n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g4DkVi3vzZv"
      },
      "source": [
        "# Create a function to evaluate your model using perplexity measurment (You can try adding other measurements as well)\n",
        "def evaluate_result(features, label, model):\n",
        "    model.evaluate(features, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwoJBrLKvzY6"
      },
      "source": [
        "### 1. Load your data "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import csv\n",
        "\n",
        "# Load data\n",
        "file = open(\"/content/pride_and_prejudice.txt\", \"r\", encoding=\"utf-8-sig\",  errors='ignore') #encode with BOM\n",
        "raw_text = file.read()"
      ],
      "metadata": {
        "id": "XX6T8lL2D481"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See raw data\n",
        "raw_text[:300]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umPc4Xr8EGxQ",
        "outputId": "47b588cf-f3ee-444a-a517-c98fa80b584a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n      It is a truth universally acknowledged, that a single man in\\n      possession of a good fortune, must be in want of a wife.\\n\\n      However little known the feelings or views of such a man may be\\n      on his first entering a neighbourhood, this truth is so well\\n      fixed in the mi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get total word and character\n",
        "chars = sorted(list(set(raw_text)))\n",
        "print(\"Total characters: \", len(chars))\n",
        "print(\"Total word: \", len(raw_text.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaWFZjt-FYy_",
        "outputId": "57abee65-2394-459a-aaec-c7304e67a7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters:  89\n",
            "Total word:  124467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qksGjaIJvzZJ"
      },
      "source": [
        "### 2. Data Preprocessing \n",
        "\n",
        "*Note that only story will be used as a dataset, footnote and creddit are not include.*\n",
        "\n",
        "The symbol '\\n' is indicated the end of the line ``<EOS>``, which is for our model to end the sentence here.\n",
        "\n",
        "To create a corpus for your model. The following code is can be used:</br>\n",
        "*Note that other techniques can be used*\n",
        "\n",
        "```python\n",
        "# cut the text in semi-redundant sequences of maxlen characters.\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "```\n",
        "\n",
        "The code loop through the data from first word to the last word. The maxlen define a next n word for a model to predict.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "RJrPnxm3F8w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding end of string symbol use .replace   to replace data_text with  [  \\n\\n', \" <EOS> \" ]\n",
        "raw_text = raw_text.replace('\\n\\n', \" <EOS> \")\n",
        "raw_text[:300]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjqeu3FKGHyh",
        "outputId": "59b1d0d0-ba3a-4b54-ad6a-b1270339bc39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1 <EOS>       It is a truth universally acknowledged, that a single man in\\n      possession of a good fortune, must be in want of a wife. <EOS>       However little known the feelings or views of such a man may be\\n      on his first entering a neighbourhood, this truth is so well\\n      fixed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create corpus & Vectorization\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "\n",
        "# basic cleanup\n",
        "corpus = raw_text.lower().split(\"\\n\")\n",
        "\n",
        "# tokenization\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# max sequence for padding \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "# Pre padding \n",
        "#input_sequences = np.array(sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# One-hot label\n",
        "label = keras.utils.to_categorical(label, num_classes=total_words)"
      ],
      "metadata": {
        "id": "j0lYj0xbGxPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find some\n",
        "#print('Max sequence len: %s' % max_sequence_len)\n",
        "print('Total word len: %s' % total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuY7Ev8VG1CM",
        "outputId": "3bcfb8f8-7e3e-4fd6-bebd-1567e08c8b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word len: 7230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_sequence[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzElSz57HlG_",
        "outputId": "0d13e739-8e78-4dd3-bc0a-ca741aae5bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7228"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictors[0]) #after padding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ecAT1L5H07u",
        "outputId": "17f7b012-1749-42ac-c32b-d52e333bc532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0 265]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(label[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bo3DHDVH5jS",
        "outputId": "b7b4f844-61d8-409d-8765-53c3c229a3da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ry6hn0vzZc"
      },
      "source": [
        "### 3. Language Model\n",
        "\n",
        "Define RNN model using LSTM and word embedding representation</br>\n",
        "We will used perplexity as a metrics\n",
        "\n",
        "```python\n",
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = keras.backend.categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = keras.backend.pow(2.0, cross_entropy)\n",
        "    return perplexity\n",
        "```\n",
        "\n",
        "To used custom metrics function > https://keras.io/metrics/\n",
        "\n",
        "For a loss function `categorical_crossentropy` is used, any optimzation method can be applied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvwPzZo8uJXv"
      },
      "source": [
        "\n",
        "In this lab, we will used perplexity as a metrics\n",
        "\n",
        "\n",
        "```\n",
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = keras.backend.categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = keras.backend.pow(2.0, cross_entropy)\n",
        "    return perplexity\n",
        "```\n",
        "\n",
        "\n",
        "To used custom metrics function > https://keras.io/metrics/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65pM7pqNvzZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0dd130-bf5a-4d48-d40a-fbab5f59d5ff"
      },
      "source": [
        "# Define your model\n",
        "\n",
        "# LSTM Layer use DEFAULT weight initializer = glorot/xavier uniform initializer,\n",
        "#            add dropout for avoid overfitting\n",
        "# If Use more than 1 LSTM layer must setting return_sequebce = True\n",
        "# Try using perplexity metrics\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Embedding(total_words, 128, input_length = max_sequence_len-1, name='Embedding'))\n",
        "model.add(layers.LSTM(128, dropout = 0.2, name = 'LSTM1'))\n",
        "model.add(layers.Dense(total_words, activation = 'softmax', name = 'Output'))\n",
        "\n",
        "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = [perplexity])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Embedding (Embedding)       (None, 60, 128)           925440    \n",
            "                                                                 \n",
            " LSTM1 (LSTM)                (None, 128)               131584    \n",
            "                                                                 \n",
            " Output (Dense)              (None, 7230)              932670    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,989,694\n",
            "Trainable params: 1,989,694\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Model\n",
        "history = model.fit(predictors, label, batch_size=128, epochs=10)\n",
        "model.save('model_baseline.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyN15YH0qS-l",
        "outputId": "275e6692-f546-4e8c-be32-0a98392ea467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "930/930 [==============================] - 16s 10ms/step - loss: 6.2994 - perplexity: 654.5432\n",
            "Epoch 2/10\n",
            "930/930 [==============================] - 10s 10ms/step - loss: 5.7359 - perplexity: 310.3341\n",
            "Epoch 3/10\n",
            "930/930 [==============================] - 10s 11ms/step - loss: 5.3554 - perplexity: 237.2975\n",
            "Epoch 4/10\n",
            "930/930 [==============================] - 10s 11ms/step - loss: 5.1197 - perplexity: 183.0687\n",
            "Epoch 5/10\n",
            "930/930 [==============================] - 10s 10ms/step - loss: 4.9360 - perplexity: 147.6357\n",
            "Epoch 6/10\n",
            "930/930 [==============================] - 10s 11ms/step - loss: 4.7835 - perplexity: 120.8282\n",
            "Epoch 7/10\n",
            "930/930 [==============================] - 10s 10ms/step - loss: 4.6505 - perplexity: 101.1184\n",
            "Epoch 8/10\n",
            "930/930 [==============================] - 9s 10ms/step - loss: 4.5345 - perplexity: 86.5859\n",
            "Epoch 9/10\n",
            "930/930 [==============================] - 10s 11ms/step - loss: 4.4250 - perplexity: 76.3589\n",
            "Epoch 10/10\n",
            "930/930 [==============================] - 9s 10ms/step - loss: 4.3256 - perplexity: 67.9782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uZwRT6RvzZu"
      },
      "source": [
        "### 4. Evaluate your model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6-pIYam3ACT",
        "outputId": "bce859df-5e4e-4514-f394-c3d3171cdcff"
      },
      "source": [
        "# Evaluate Model\n",
        "model = models.load_model('/content/model_baseline.h5', custom_objects = {'perplexity': perplexity})\n",
        "evaluate_result(predictors, label, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3720/3720 [==============================] - 24s 5ms/step - loss: 4.1282 - perplexity: 52.4014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. First Discussion and This Experiment"
      ],
      "metadata": {
        "id": "GpmChEpF9myO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary of my understanding about perplexity, perplexity is a metric about probability of words and entropy, and it's the better to evaluate than loss/accuracy. The lower perplexity refers to better language model. According to the first model \"model_baseline\", evaluated perplexity is 52.4014. It's quite low.\n",
        "\n",
        "In this experiment, I will use perplexity to evaluate models, and use tuning some hyperparameter to find the different results."
      ],
      "metadata": {
        "id": "W7Km0yB342kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tuning Hyperparameter"
      ],
      "metadata": {
        "id": "LGJmutysBLvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Add Second LSTM Layer\n",
        "My assumption is that model is better (perplexity is lower)."
      ],
      "metadata": {
        "id": "JIsEAc-OBQ41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model\n",
        "\n",
        "# LSTM Layer use DEFAULT weight initializer = glorot/xavier uniform initializer,\n",
        "#            add dropout for avoid overfitting\n",
        "# If Use more than 1 LSTM layer must setting return_sequences = True\n",
        "\n",
        "model2 = models.Sequential()\n",
        "model2.add(layers.Embedding(total_words, 128, input_length = max_sequence_len-1, name='Embedding'))\n",
        "model2.add(layers.LSTM(128, dropout = 0.2, return_sequences = True, name = 'LSTM1'))\n",
        "model2.add(layers.LSTM(128, dropout = 0.2, name = 'LSTM2'))\n",
        "model2.add(layers.Dense(total_words, activation = 'softmax', name = 'Output'))\n",
        "\n",
        "model2.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = [perplexity])\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwutJos8GxjU",
        "outputId": "af1aa9e8-c137-48c9-d0b9-6f4888a59915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Embedding (Embedding)       (None, 60, 128)           925440    \n",
            "                                                                 \n",
            " LSTM1 (LSTM)                (None, 60, 128)           131584    \n",
            "                                                                 \n",
            " LSTM2 (LSTM)                (None, 128)               131584    \n",
            "                                                                 \n",
            " Output (Dense)              (None, 7230)              932670    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,121,278\n",
            "Trainable params: 2,121,278\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Model\n",
        "history = model2.fit(predictors, label, batch_size=128, epochs=10)\n",
        "model2.save('model_2LSTM.h5')"
      ],
      "metadata": {
        "id": "ZJ1KVeZ4HIxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "model = models.load_model('/content/model_2LSTM.h5', custom_objects = {'perplexity': perplexity})\n",
        "evaluate_result(predictors, label, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3Qm28ScI_rk",
        "outputId": "87757fba-cc67-4c2d-d537-4c3a25b4e85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3720/3720 [==============================] - 30s 6ms/step - loss: 4.5668 - perplexity: 71.4377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perplexity of \"model_2LSTM\" is 71.4377 (more than 1 layer LSTM). It's not result that I expected. After research a few simple language models with LSTM, there is no model that have more than 1 LSTM layer. I think it's not necessary process(it's not matter) or over-processing. "
      ],
      "metadata": {
        "id": "8ydLEL2hKA47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####No Dropout \n",
        "This tuning is rather sure to improve the model, but let's see."
      ],
      "metadata": {
        "id": "HGnkPEdRL_d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model\n",
        "# LSTM Layer use DEFAULT weight initializer = glorot/xavier uniform initializer,\n",
        "\n",
        "model3 = models.Sequential()\n",
        "model3.add(layers.Embedding(total_words, 128, input_length = max_sequence_len-1, name='Embedding'))\n",
        "model3.add(layers.LSTM(128, name = 'LSTM1'))\n",
        "model3.add(layers.Dense(total_words, activation = 'softmax', name = 'Output'))\n",
        "\n",
        "model3.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = [perplexity])\n",
        "model3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfhdtMVAMzh6",
        "outputId": "00e38063-2c03-4189-b19a-1da739f289c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Embedding (Embedding)       (None, 60, 128)           925440    \n",
            "                                                                 \n",
            " LSTM1 (LSTM)                (None, 128)               131584    \n",
            "                                                                 \n",
            " Output (Dense)              (None, 7230)              932670    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,989,694\n",
            "Trainable params: 1,989,694\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Model\n",
        "history = model3.fit(predictors, label, batch_size=128, epochs=10)\n",
        "model3.save('model_noDrop.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzoRSIVbNDR8",
        "outputId": "f4caf061-80ab-4792-9f50-b8dd6ed218eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "930/930 [==============================] - 16s 11ms/step - loss: 6.2934 - perplexity: 649.1934\n",
            "Epoch 2/10\n",
            "930/930 [==============================] - 10s 11ms/step - loss: 5.6716 - perplexity: 299.9314\n",
            "Epoch 3/10\n",
            "930/930 [==============================] - 10s 10ms/step - loss: 5.3027 - perplexity: 227.7965\n",
            "Epoch 4/10\n",
            "930/930 [==============================] - 10s 10ms/step - loss: 5.0756 - perplexity: 176.0975\n",
            "Epoch 5/10\n",
            "930/930 [==============================] - 10s 10ms/step - loss: 4.8942 - perplexity: 141.2868\n",
            "Epoch 6/10\n",
            "930/930 [==============================] - 10s 10ms/step - loss: 4.7400 - perplexity: 115.4907\n",
            "Epoch 7/10\n",
            "930/930 [==============================] - 10s 10ms/step - loss: 4.6025 - perplexity: 96.6701\n",
            "Epoch 8/10\n",
            "930/930 [==============================] - 10s 10ms/step - loss: 4.4775 - perplexity: 82.2302\n",
            "Epoch 9/10\n",
            "930/930 [==============================] - 9s 10ms/step - loss: 4.3611 - perplexity: 71.7442\n",
            "Epoch 10/10\n",
            "930/930 [==============================] - 9s 10ms/step - loss: 4.2511 - perplexity: 64.1100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "model = models.load_model('/content/model_noDrop.h5', custom_objects = {'perplexity': perplexity})\n",
        "evaluate_result(predictors, label, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gltwlsZ0OLqJ",
        "outputId": "1cae96b7-c71f-43b3-df0c-4b30b52300d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3720/3720 [==============================] - 23s 5ms/step - loss: 4.0681 - perplexity: 51.0891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perplexity of \"model_2LSTM\" is 51.0891. It's a little less than the baseline model, and maybe not significant. \n",
        "\n",
        "Now, I'm quite confident that this architecture (embedding, 1 layer of LSTM, and Dense for output) is fine. There is still lots of hyperparameter that can tune, but I think tuning regularizer is not help. Moreover, there is no massive specific reason that to tune initializer, constraint, and optimizer. \n",
        "\n",
        "In the next experiment, I will try increase number of LSTM units and decrease batch size to see how much significant change the perplexity."
      ],
      "metadata": {
        "id": "wcACkTBxPHrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Increase LSTM Units"
      ],
      "metadata": {
        "id": "836gmM_-huEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model\n",
        "# LSTM Layer use DEFAULT weight initializer = glorot/xavier uniform initializer,\n",
        "# No dropout\n",
        "# Increase LSTM units x2 (128 -> 256)\n",
        "# If increase LSTM -> must increase output dimension of embedding layer too\n",
        "\n",
        "model4 = models.Sequential()\n",
        "model4.add(layers.Embedding(total_words, 256, input_length = max_sequence_len-1, name='Embedding'))\n",
        "model4.add(layers.LSTM(256, name = 'LSTM'))\n",
        "model4.add(layers.Dense(total_words, activation = 'softmax', name = 'Output'))\n",
        "\n",
        "model4.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = [perplexity])\n",
        "model4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BR_BBmdhoe3",
        "outputId": "9076994f-f25c-41dc-94f3-4b8aedf02523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Embedding (Embedding)       (None, 60, 256)           1850880   \n",
            "                                                                 \n",
            " LSTM (LSTM)                 (None, 256)               525312    \n",
            "                                                                 \n",
            " Output (Dense)              (None, 7230)              1858110   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,234,302\n",
            "Trainable params: 4,234,302\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Model\n",
        "history = model4.fit(predictors, label, batch_size=128, epochs=10)\n",
        "model4.save('model_increUnit.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gytp1bDsiI0O",
        "outputId": "77c1c2df-5d06-4a45-bdf3-21e4ee015ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "930/930 [==============================] - 20s 15ms/step - loss: 6.1288 - perplexity: 685.9318\n",
            "Epoch 2/10\n",
            "930/930 [==============================] - 13s 14ms/step - loss: 5.3582 - perplexity: 300.4081\n",
            "Epoch 3/10\n",
            "930/930 [==============================] - 13s 14ms/step - loss: 4.9892 - perplexity: 179.6489\n",
            "Epoch 4/10\n",
            "930/930 [==============================] - 13s 14ms/step - loss: 4.7143 - perplexity: 119.7724\n",
            "Epoch 5/10\n",
            "930/930 [==============================] - 13s 14ms/step - loss: 4.4826 - perplexity: 85.9898\n",
            "Epoch 6/10\n",
            "930/930 [==============================] - 14s 15ms/step - loss: 4.2701 - perplexity: 65.9632\n",
            "Epoch 7/10\n",
            "930/930 [==============================] - 15s 16ms/step - loss: 4.0687 - perplexity: 53.0690\n",
            "Epoch 8/10\n",
            "930/930 [==============================] - 14s 15ms/step - loss: 3.8719 - perplexity: 43.8041\n",
            "Epoch 9/10\n",
            "930/930 [==============================] - 13s 14ms/step - loss: 3.6844 - perplexity: 37.5160\n",
            "Epoch 10/10\n",
            "930/930 [==============================] - 13s 14ms/step - loss: 3.5063 - perplexity: 33.2219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "model = models.load_model('/content/model_increUnit.h5', custom_objects = {'perplexity': perplexity})\n",
        "evaluate_result(predictors, label, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufqf699Uiek_",
        "outputId": "a1d4f7a4-1fbb-4d7f-a5c2-963b58b9b9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3720/3720 [==============================] - 23s 5ms/step - loss: 3.2083 - perplexity: 25.0206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After increasing LSTM units 2 times, total parameters and size of model are almost double too. Perplexity of this new model is 25.0206. It's dramatically improve."
      ],
      "metadata": {
        "id": "EoQgV-PmjMtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decrease Batch Size"
      ],
      "metadata": {
        "id": "-TAiX8xelPx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model\n",
        "# LSTM Layer use DEFAULT weight initializer = glorot/xavier uniform initializer,\n",
        "# No dropout\n",
        "# Increase LSTM units x2 (128 -> 256)\n",
        "# If increase LSTM -> must increase output dimension of embedding layer too\n",
        "\n",
        "model5 = models.Sequential()\n",
        "model5.add(layers.Embedding(total_words, 256, input_length = max_sequence_len-1, name='Embedding'))\n",
        "model5.add(layers.LSTM(256, name = 'LSTM'))\n",
        "model5.add(layers.Dense(total_words, activation = 'softmax', name = 'Output'))\n",
        "\n",
        "model5.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = [perplexity])\n",
        "model5.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM9hp8nQleji",
        "outputId": "1c783b52-0b73-466b-9083-d24c121a6a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Embedding (Embedding)       (None, 60, 256)           1850880   \n",
            "                                                                 \n",
            " LSTM (LSTM)                 (None, 256)               525312    \n",
            "                                                                 \n",
            " Output (Dense)              (None, 7230)              1858110   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,234,302\n",
            "Trainable params: 4,234,302\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Model\n",
        "# Reduce batch size from 128 to 64\n",
        "history = model5.fit(predictors, label, batch_size=64, epochs=10)\n",
        "model5.save('model_increUnit_reduSize.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru2XdRffsD64",
        "outputId": "66b84fc6-bb5c-4181-cd8a-3d743da3b0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1860/1860 [==============================] - 25s 10ms/step - loss: 5.9658 - perplexity: 697.8634\n",
            "Epoch 2/10\n",
            "1860/1860 [==============================] - 23s 12ms/step - loss: 5.1481 - perplexity: 260.9930\n",
            "Epoch 3/10\n",
            "1860/1860 [==============================] - 20s 11ms/step - loss: 4.7593 - perplexity: 136.7756\n",
            "Epoch 4/10\n",
            "1860/1860 [==============================] - 19s 10ms/step - loss: 4.4358 - perplexity: 84.3082\n",
            "Epoch 5/10\n",
            "1860/1860 [==============================] - 19s 10ms/step - loss: 4.1291 - perplexity: 57.5445\n",
            "Epoch 6/10\n",
            "1860/1860 [==============================] - 19s 10ms/step - loss: 3.8307 - perplexity: 42.9745\n",
            "Epoch 7/10\n",
            "1860/1860 [==============================] - 19s 10ms/step - loss: 3.5429 - perplexity: 33.5638\n",
            "Epoch 8/10\n",
            "1860/1860 [==============================] - 19s 10ms/step - loss: 3.2707 - perplexity: 27.5337\n",
            "Epoch 9/10\n",
            "1860/1860 [==============================] - 19s 10ms/step - loss: 3.0151 - perplexity: 23.2480\n",
            "Epoch 10/10\n",
            "1860/1860 [==============================] - 19s 10ms/step - loss: 2.7790 - perplexity: 19.9350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "model = models.load_model('/content/model_increUnit_reduSize.h5', custom_objects = {'perplexity': perplexity})\n",
        "evaluate_result(predictors, label, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NV3IS5asrTx",
        "outputId": "f5b0ba3f-5de8-4b7c-b4a3-e8c583f44e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3720/3720 [==============================] - 23s 5ms/step - loss: 2.3694 - perplexity: 13.9848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexity is 13.9848. It's reduced almost 2 times following my assumption. Then, I will use this model to text generating in the next section."
      ],
      "metadata": {
        "id": "Zo78GsgesvXj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCU3FZjsvzZy"
      },
      "source": [
        "### 6. Text generating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAd1Kdl3vzZz"
      },
      "source": [
        "def generate_text(seedtext, next_words, max_sequence_len, model):\n",
        "  for j in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seedtext])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    #predicted = model.predict_classes(token_list, verbose=0)\n",
        "    predict_x = model.predict(token_list) \n",
        "    predicted = np.argmax(predict_x,axis=1)\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == predicted:\n",
        "        output_word = word\n",
        "        break\n",
        "    seedtext +=\" \" + output_word\n",
        "  return seedtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model = models.load_model('/content/model_increUnit_reduSize.h5', custom_objects = {'perplexity': perplexity})"
      ],
      "metadata": {
        "id": "-VScUpnrwINy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC3AVscmvzZ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "f256ba38-32c0-4848-ac0c-9be7c282a9a3"
      },
      "source": [
        "# Generate your sample text\n",
        "seed_text = input('Enter your start sentence:')\n",
        "gen_text = generate_text(seed_text, 10, max_sequence_len, model)\n",
        "gen_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your start sentence:We are\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We are very glad to be done for my own and pray'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate your sample text\n",
        "seed_text = input('Enter your start sentence:')\n",
        "gen_text = generate_text(seed_text, 10, max_sequence_len, model)\n",
        "gen_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "1zUzbB8PwAu_",
        "outputId": "561a69e6-6584-49a6-f33c-8e9409660a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your start sentence:She can\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'She can herself be the only probable person for the first object'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate your sample text\n",
        "seed_text = input('Enter your start sentence:')\n",
        "gen_text = generate_text(seed_text, 20, max_sequence_len, model)\n",
        "gen_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "Az9-9kjdyhVO",
        "outputId": "58252c64-b279-4abf-fb44-ee89c521e39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your start sentence:You should\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You should not have a more promising for him ‚Äù eos elizabeth was then sorry for her and the two youngest repaired'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate your sample text\n",
        "seed_text = input('Enter your start sentence:')\n",
        "gen_text = generate_text(seed_text, 15, max_sequence_len, model)\n",
        "gen_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "hFSk61O5ysDj",
        "outputId": "2244c683-284c-4e77-faff-1b133fa5548d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your start sentence:Marriage is\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Marriage is a very good match for a young man who has been very much in love'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Discussion\n",
        "After try some sample, sentences that are generated from given words are reasonable. We can see the meaning of them. Pride&Prejudice is a legendary novel that are uniqly portrayal. It has been praised about charming characters and usage of language. The results are showed portrayal too."
      ],
      "metadata": {
        "id": "kxOEUn6Py2r7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNoHjlejvzaB"
      },
      "source": [
        "###[Speacial] 7. Help your model to generate a short story \n",
        "\n",
        "**Example** https://medium.com/deep-writing/harry-potter-written-by-artificial-intelligence-8a9431803da6\n",
        "\n",
        "Write your result in a `markdown` cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9tjrUmHvzaC"
      },
      "source": [
        "# Create your short-story from your model (Add your creativity here)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLz1ABUgvzaI"
      },
      "source": [
        "### More on Natural language Processing and Language model\n",
        "1. https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e \n",
        "2. https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b\n",
        "3. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "**Music generates by RNN**\n",
        "https://soundcloud.com/optometrist-prime/recurrence-music-written-by-a-recurrent-neural-network\n"
      ]
    }
  ]
}